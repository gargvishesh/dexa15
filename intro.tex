\section{Introduction}
\label{sec:intro}
%
Phase Change Memory (PCM) is a recently developed non-volatile memory
technology, constructed from chalcogenide glass material, that stores
data by switching between crystalline (\emph{binary 0}) and amorphous 
(\emph{binary 1}) states. Broadly speaking, it is expected to provide an attractive
combination of the best features of conventional disks (persistence,
capacity) and of DRAM (access speed). For instance, it is
about 2 to 4 times denser than DRAM, while providing a DRAM-comparable
read latency.  On the other hand, it consumes much less energy
than magnetic hard disks while providing substantively smaller write latency. Due to this suite of  desirable features, PCM technology is
expected to play a prominent role in the next generation of computing
systems, either augmenting or replacing current components in the memory
hierarchy~\cite{qureshi,zhou,lee}.

A limitation of PCM, however, is that there is a significant difference
between the read and write behaviours in terms of energy, latency and
bandwidth. A PCM write, for example, consumes 6 times more energy than
a read. Moreover, PCM has limited write endurance since a memory cell
becomes unusable after the number of writes to the cell exceed a threshold
determined by the underlying glass material. Consequently, several database
researchers have, in recent times, focused their attention on devising
new implementations of the core database operators that are adapted to
the idiosyncrasies of the PCM environment (e.g.~\cite{chen,viglas}). 

%\newpage
\subsection*{Architectural Model}
The prior database work has primarily focused on computing
architectures wherein either (a) PCM completely replaces the
DRAM memory~\cite{chen}; or (b) PCM and DRAM co-exist side-by-side
and are independently controlled by the software~\cite{viglas}. We
hereafter refer to these options as {\bf \modelPcmRam{}} and
{\bf \modelExplicit{}}, respectively.

However, a third option that is gaining favor in the architecture
community is where the PCM is augmented with a small hardware-managed
DRAM buffer~\cite{qureshi}. In this model, which we refer to as
{\bf \model{}}, the address space of the application maps to PCM and the
DRAM buffer can simply be visualized as yet another level of the existing
cache hierarchy.  For ease of comparison, these various configurations
are pictorially shown in Figure~\ref{fig:pcm_models}.

\begin{figure}[htbp]
	\includegraphics[height=50mm]{PCM_Models.png}\centering
	\caption{PCM-based Architectural Options}
	\label{fig:pcm_models}
\end{figure}

There are several practical advantages of the \model{} configuration:
First, the write latency drawback of \modelPcmRam{} can be largely
concealed by the intermediate DRAM buffer~\cite{qureshi}. Second,
existing applications can be used \textit{as is} but still manage to take
advantage of both the DRAM and the PCM. This is in stark contrast to the
\modelExplicit{} model which requires incorporating additional machinery,
either in the program or in the OS, to distinguish between data mapped
to DRAM and to PCM -- for example, by having separate address space
mappings for the different memories.

\subsection*{Our Work}
We make the following contributions in this paper:
\begin{itemize}
\item Propose PCM-conscious off-the-shelf implementations of the ``workhorse'' database operators:
\textit{sort}, \textit{hash join} and \textit{group-by}, that are tuned to the \model{} model, that reduce
\emph{both} query writes and response times.

\item Incorporate major extensions in Multi2sim \cite{multi2sim},
a state-of-the-art architectural simulator, to support the modelling of PCM.  



\item Evaluate the proposed implementation on \emph{complete} TPC-H benchmark queries.This is a noteworthy point since earlier studies of PCM databases had only
considered operator performance in isolation.  But, it is possible that
optimizing a specific operator may be detrimental to other downstream
operators that follow it in the query execution plan. For instance,
the proposal in \cite{chen} to keep nodes unsorted in B$^+$ indexes,
while saving on writes, can be detrimental to the running times of
subsequent operators, e.g. join filters, that leverage index ordering. The metric of \emph{wear  distribution} is also included in our evaluation.


\item Provide simple but effective mathematical \emph{estimators} for the
number of writes incurred by the new operators and then incorporate them in the query optimizer's
cost model.

\end{itemize}

We observe that the alternate implementations collectively offer
substantive benefits with regard to PCM writes -- the number is typically
brought down \emph{by a factor of two to three}.  Concurrently, the query
response times are also brought down by about \emph{20 to 30 percent}. Overall, these outcomes augur well for the impending migration of database
engines to PCM-based computing platforms.

 
 
 
\begin{comment}
 Our 

The experimental results suggest that, as compared to the original
PCM-oblivious operators, 
As a sample case in point, for TPC-H Query 19, savings of 64\% in PCM
writes are achieved with a concomitant 32\% reduction in CPU cycles.

A unique feature of our analysis is that we also . This is because
a skewed distribution adversely impacts the endurance of cells with high
write rates, raising the frequency at which wear-leveling mechanisms
have to be put into play by the system. We observe that our algorithms don't introduce any new skew while achieving the reduction in writes and cycles.

Finally, 

In a nutshell, the new PCM-conscious operators proposed in this paper
provide both \emph{short-term and long-term performance benefits}.

\end{comment}